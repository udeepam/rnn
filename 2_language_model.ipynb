{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling using a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "* [Sequence to Sequence Learning with Neural Networks by Ben Trevett](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)\n",
    "* [Sequence-tosequence modelling with nn.Tranformer and TorchText by PyTorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "* [A Comprehensive Introduction to Torchtext (Practical Torchtext part 1) by keitakurita](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)\n",
    "* [Language modeling tutorial in torchtext by keitakurita](https://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/)\n",
    "* [Language Models and Contextualised Word Embeddings by David S. Batista](http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import copy\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data import Field, BPTTIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a `torch.device` which is used to tell PyTorch to put the tensors on the GPU or not. We use the `torch.cuda.is_available()` function, which will return True if a GPU is detected on our computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd7a76189d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix the random seeds for reproducibility\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modelling \n",
    "\n",
    "**TODO:** Explain what language modelling is probabilisitically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the Data\n",
    "\n",
    "`torchtext.data.Field` handles how the data should be processed, the arguments for which can be found [here](https://pytorch.org/text/data.html). We set the `tokenize` argument to \"basic english\". The Field also appends the \"start of sequence\" and \"end of sequence\" tokens via the `init_token` and `eos_token` arguments, and converts all words to lowercase.\n",
    "\n",
    "A tokenizer is used to turn a string containing a sentence into a list of individual tokens that make up that string, e.g. \"$\\texttt{good morning!}$\" becomes [\"$\\texttt{good}$\", \"$\\texttt{morning}$\", \"$\\texttt{!}$\"]. We'll talk about the sentences being a sequence of tokens, instead of saying they're a sequence of words. This is because \"$\\texttt{good}$\" and \"$\\texttt{morning}$\" are both words and tokens, but \"$\\texttt{!}$\" is a token, not a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/udeepa/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# define how data should look\n",
    "TEXT = Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "             init_token='<sos>',\n",
    "             eos_token='<eos>',\n",
    "             lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and load the train, validation, test datasets from WikiText-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "train_x, val_x, test_x = WikiText2.splits(TEXT)\n",
    "print(f\"Number of training words:   {len(train_x.examples[0].text)}\")\n",
    "print(f\"Number of validation words: {len(val_x.examples[0].text)}\")\n",
    "print(f\"Number of testing words:    {len(test_x.examples[0].text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next build the vocabulary for our language modelling task. The vocabulary is used to associate each unique token with an index (an integer). It is important to note that our vocabulary should only be built from the training set and not the validation/test set. This prevents \"information leakage\" into our model, giving us artifically inflated validation/test scores.\n",
    "\n",
    "Using the `min_freq` argument, we only allow tokens that appear at least 2 times to be in our vocabulary. Tokens that appear only once are converted into an \"$\\texttt{<unk>}$\" (unknown) token. Additionally, with the `vectors` argument we can use precomputed static word embeddings, in this case GloVe vectors with 200 dimensions:\n",
    "    \n",
    "* **Static word embeddings** map each token to a single vector, e.g. Skip-Gram (word2vec), GloVe, fastText. This means that the same word will have the same embedding regardless of the context it occurs in.\n",
    "    \n",
    "* **Contextual word embeddings** take into consideration the context of words, such that a word may have a different embedding depending on the words surrounding it e.g. BERT, ELMO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_precomputed_embeds = False\n",
    "batch_size = 64\n",
    "seq_len = 30\n",
    "\n",
    "# build vocabulary from the training data\n",
    "TEXT.build_vocab(train_x, \n",
    "                 min_freq=2, \n",
    "                 vectors=\"glove.6B.200d\" if use_precomputed_embeds else None)\n",
    "vocab_size = len(TEXT.vocab.stoi)\n",
    "print(f\"Number of unique tokens in vocabulary: {len(TEXT.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the iterator:\n",
    "* Divides the corpus into batches of sequence length => [`bppt_len`, `batch_size`].\n",
    "* Does not use padding and so drops the left over data after batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BPTTIterator.splits((train_x, val_x, test_x),\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      bptt_len=seq_len,\n",
    "                                                      device=device,\n",
    "                                                      repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "* Here in the forward pass we keep the hidden state of the RNN because the entire dataset is a continuous corpus, meaning we want to retain the hidden state between sequences within a batch. \n",
    "\n",
    "* We can't retain the entire history as this would mean the model will try to backpropogate to the beginning of the dataset which requires a lot of memory and time. Hence we have a `repackage_hidden()` function used at the start of each batch, which treats the hidden state of the RNN from the previous batch as a constant.\n",
    "\n",
    "* We initialise all the weights of the network from a uniform distribution, $\\mathcal{U}(-0.1,0.1)$ and initialise the biases to $0$.\n",
    "\n",
    "* Additionally, we have a `init_hidden()` function which initialises the hidden state of the RNN to a zero vector. Remember if using an LSTM then we have both the hidden state and the cell state to initialise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 output_size,\n",
    "                 embed_size,\n",
    "                 hidden_size,\n",
    "                 num_layers,\n",
    "                 mlp_layer_size=None,\n",
    "                 precomputed_embeds=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : `str`\n",
    "            Recurrent layer type {rnn, gru, lstm}.\n",
    "        output_size : `int``\n",
    "            Number of unique words and symbols in the texts, i.e. the vocab_size.\n",
    "        embed_size : `int``\n",
    "            The size of the embedding space.\n",
    "        hidden_size : `int`\n",
    "            The number of features in the hidden state.\n",
    "        num_layers : `int`\n",
    "            Number of recurrent layers. e.g. setting n_layers=2\n",
    "            would mean stacking two RNNs together to form a stacked RNN,\n",
    "            with the second RNN taking in outputs of the first RNN and\n",
    "            computing the final results.\n",
    "        mlp_layer_size : `list`\n",
    "            The number of hidden layers for the downstream MLP.\n",
    "        precomputed_embeds: `numpy.ndarray`\n",
    "            Precomputed weight matrix for embedding layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model.lower()\n",
    "        self.output_size = output_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.mlp_layer_size = mlp_layer_size\n",
    "\n",
    "        # define embedding layer\n",
    "        self.encoder = nn.Embedding(num_embeddings=output_size, \n",
    "                                    embedding_dim=embed_size)\n",
    "\n",
    "        # define Recurrent Layers\n",
    "        if self.model == \"rnn\":\n",
    "            self.rnn = nn.RNN(input_size=embed_size,\n",
    "                              hidden_size=hidden_size,\n",
    "                              num_layers=num_layers,\n",
    "                              batch_first=True)\n",
    "        elif self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(input_size=embed_size,\n",
    "                              hidden_size=hidden_size,\n",
    "                              num_layers=num_layers,\n",
    "                              batch_first=True)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(input_size=embed_size,\n",
    "                               hidden_size=hidden_size,\n",
    "                               num_layers=num_layers,\n",
    "                               batch_first=True)\n",
    "\n",
    "        # define decoder: MLP and/or fully connected layer\n",
    "        if mlp_layer_size is not None:\n",
    "            self.mlp = nn.ModuleList()\n",
    "            for i, layer_size in enumerate(mlp_layer_size):\n",
    "                if i == 0:\n",
    "                    self.mlp.append(nn.Linear(in_features=hidden_size,\n",
    "                                              out_features=layer_size))\n",
    "                else:\n",
    "                    self.mlp.append(nn.Linear(in_features=mlp_layer_size[i-1],\n",
    "                                              out_features=layer_size))\n",
    "            # define fully connected ouput Layer\n",
    "            self.fc = nn.Linear(in_features=mlp_layer_size[-1],\n",
    "                                out_features=output_size)\n",
    "        else:\n",
    "            # define fully connected ouput Layer\n",
    "            self.fc = nn.Linear(in_features=hidden_size,\n",
    "                                out_features=output_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # initialise the weights and biases of the network\n",
    "        self.init_weights()\n",
    "\n",
    "        # initialise the embedding layer with the precomputed weights\n",
    "        if precomputed_embeds is not None:\n",
    "            self.encoder.weight.data.copy_(precomputed_embeds)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input : `torch.Tensor`\n",
    "            Input to the model, [batch_size, seq_len].\n",
    "        hidden : `torch.Tensor` or `tuple` of `torch.Tensor`\n",
    "            \"rnn\" & \"gru\" = hidden state, [num_layers, batch_size, hidden_size],\n",
    "            \"lstm\" = tuple of hidden_state and cell_state, both [num_layers, batch_size, hidden_size].\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        x : `torch.Tensor`\n",
    "            Output of predictions, [batch_size, seq_len, output_size].\n",
    "        hidden : `torch.Tensor` or `tuple` of `torch.Tensor`\n",
    "            \"rnn\" & \"gru\" = hidden state, [num_layers, batch_size, hidden_size],\n",
    "            \"lstm\" = tuple of hidden_state and cell_state, both [num_layers, batch_size, hidden_size].\n",
    "        \"\"\"\n",
    "        # encode the input using embedding layer\n",
    "        x = self.encoder(input) # [batch_size, seq_len, embed_size]\n",
    "        # pass through RNN\n",
    "        x, hidden = self.rnn(x, hidden) # [batch_size, seq_len, hidden_size] and [num_layers, batch_size, hidden_size]       \n",
    "        \n",
    "        # pass through MLP\n",
    "        if self.mlp_layer_size is not None:\n",
    "            for i in range(len(self.mlp)):\n",
    "                x = self.relu(self.mlp[i](x))\n",
    "\n",
    "        # pass through final fully connected output layer\n",
    "        \"\"\"\n",
    "        No activation function is applied here during training\n",
    "        because the cross entropy loss function applies one for us.\n",
    "        However when predicting we will need to put a softmax layer on top\n",
    "        to obatin a probability distribution.\n",
    "        \"\"\"\n",
    "        x = self.fc(x) # [batch_size, seq_len, output_size]   \n",
    "        return x, hidden\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialise the weights and biases of the network.\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Initialise the hidden state of the RNN to zeros.\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.model==\"lstm\":\n",
    "            return (Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device)),\n",
    "                    Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_()).to(device))\n",
    "        else:\n",
    "            return Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device))\n",
    "\n",
    "    def repackage_hidden(self, hidden):\n",
    "        \"\"\"\n",
    "        Reset the hidden state of the RNN.\n",
    "        Wraps hidden states in new Variables, to detach them from their history.\n",
    "\n",
    "        https://discuss.pytorch.org/t/solved-why-we-need-to-detach-variable-which-contains-hidden-representation/1426/13\n",
    "        https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226/6\n",
    "        \"\"\"\n",
    "        if self.model==\"lstm\":\n",
    "            return tuple(Variable(v.data) for v in hidden)\n",
    "        else:\n",
    "            return Variable(hidden.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We now define the training procedure:\n",
    "* We use the `torch.nn.CrossEntropyLoss` function. This loss takes the index of the correct class as the ground truth instead of a one-hot vector. Unfortunately, it only takes tensors of dimension 2 so we need to rehape our prediction and target matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          train_iter, \n",
    "          val_iter, \n",
    "          epochs, \n",
    "          batch_size, \n",
    "          learning_rate, \n",
    "          clip,\n",
    "          solver,\n",
    "          device):\n",
    "    \"\"\"\n",
    "    Fit model with the training data and validate using the validation data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : `torch.nn.Module` \n",
    "        The model.\n",
    "    train_iter : `torchtext.data.iterator.BPTTIterator`\n",
    "        The training data iterator.\n",
    "    val_iter : `torchtext.data.iterator.BPTTIterator`\n",
    "        The validation data iterator.\n",
    "    batch_size : `int`\n",
    "        The number of sequences per batch.\n",
    "    learning_rate : `float`\n",
    "        The learning rate.\n",
    "    clip : `float`\n",
    "        Value for clipping the gradients by.\n",
    "    solver : `str`\n",
    "        The optimiser to use, either \"adam\" or \"sgd\".\n",
    "    device : `torch.device`\n",
    "        Whether working on GPU or CPU.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    measures : `dict` of `list`s\n",
    "        Dictionary of training and validation \n",
    "        accuracies and losses for each epoch.\n",
    "    \"\"\"  \n",
    "    # send model to gpu or cpu\n",
    "    model.to(device)\n",
    "    # set model to train mode    \n",
    "    model.train()        \n",
    "\n",
    "    # initialise optimiser\n",
    "    if solver.lower() == \"adam\": \n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif solver.lower == \"sgd\":\n",
    "        optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    # initialise loss function: cross-entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()     \n",
    "    \n",
    "    # initialise the hidden state of the RNN to zeros\n",
    "    hidden = model.init_hidden(batch_size, device)       \n",
    "    \n",
    "    measures = defaultdict(list)\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_val = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        train_loss = train_corr = 0.\n",
    "\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            # detach hidden state from history\n",
    "            hidden = model.repackage_hidden(hidden)            \n",
    "            \n",
    "            x, y = batch.text.T, batch.target.T # [batch_size, seq_len]\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # forward Propogation: get prediction from model\n",
    "            preds, hidden = model(x, hidden) # [batch_size, seq_len, output_size] and [num_layers, batch_size, hidden_size]         \n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = criterion(preds.reshape(-1, model.output_size), \n",
    "                             y.reshape(-1))              \n",
    "       \n",
    "            # backpropogation: calculating gradients\n",
    "            loss.backward()\n",
    "            # prevent the exploding gradient problem in RNNs/LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            # update weights\n",
    "            optimiser.step()\n",
    "            \n",
    "            # append the train loss\n",
    "            train_loss += loss.item()     \n",
    "            # get number of tokens correct \n",
    "            preds = preds.argmax(2).cpu().numpy()\n",
    "            y = y.cpu().numpy()\n",
    "            train_corr += int((preds == y).sum())  \n",
    "\n",
    "        # set model to evaluate mode\n",
    "        model.eval() \n",
    "        # get new hidden states for RNN \n",
    "        val_hidden = model.init_hidden(batch_size, device)           \n",
    "        # monitor the validation loss\n",
    "        val_loss = val_corr = 0.        \n",
    "        with torch.no_grad():\n",
    "            for j, batch in enumerate(val_iter):                    \n",
    "                x, y = batch.text.T, batch.target.T # [batch_size, seq_len]\n",
    "                # get validation prediction\n",
    "                preds, val_hidden = model(x, val_hidden) # [batch_size, seq_len, output_size] and [num_layers, batch_size, hidden_size]          \n",
    "                # get validaiton loss\n",
    "                loss = criterion(preds.reshape(-1, model.output_size), \n",
    "                                 y.reshape(-1)) \n",
    "                # add the validation loss\n",
    "                val_loss += loss.item()     \n",
    "                # get number of tokens correct  \n",
    "                preds = preds.argmax(2).cpu().numpy()\n",
    "                y = y.cpu().numpy()\n",
    "                val_corr += int((preds == y).sum()) \n",
    "\n",
    "        # put model to train mode\n",
    "        model.train()      \n",
    "        \n",
    "        # save losses from epoch\n",
    "        train_loss /= len(train_iter)\n",
    "        val_loss /= len(val_iter)\n",
    "        measures[\"Train loss\"].append(train_loss)\n",
    "        measures[\"Val loss\"].append(val_loss)\n",
    "        # save accuracies from epoch\n",
    "        train_corr /= len(train_iter.dataset.examples[0].text)\n",
    "        val_corr /= len(val_iter.dataset.examples[0].text)      \n",
    "        measures[\"Train acc\"].append(train_corr)\n",
    "        measures[\"Val acc\"].append(val_corr)        \n",
    "        \n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs), \"|\", \"Time Taken: {:,.4f} seconds\".format(time.time()-start_time)) \n",
    "        print(\"\\tTrain Loss: {:.4f}\".format(train_loss), \"|\", \"Val Loss: {:.4f}\".format(val_loss))\n",
    "        print(\"\\tTrain PPL:  {:.4f}\".format(math.exp(train_loss)), \"|\", \"Val PPL:  {:.4f}\".format(math.exp(val_loss)))\n",
    "        print(\"\\tTrain Acc:  {:.4f}\".format(train_corr), \"|\", \"Val Acc:  {:.4f}\".format(val_corr)) \n",
    "\n",
    "        # save best model\n",
    "        if measures[\"Val loss\"][-1] < best_val:\n",
    "            best_val = measures[\"Val loss\"][-1]\n",
    "            best_model = copy.deepcopy(model)\n",
    "    \n",
    "    return best_model, measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of model\n",
    "embed_size = 200\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "mlp_layer_size=None\n",
    "\n",
    "# hyperparameters for training\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "clip = 5\n",
    "solver = \"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(model=\"lstm\",\n",
    "          output_size=vocab_size,\n",
    "          embed_size=embed_size,\n",
    "          hidden_size=hidden_size, \n",
    "          num_layers=num_layers, \n",
    "          mlp_layer_size=mlp_layer_size,\n",
    "          precomputed_embeds=Text.vocab.vectors if use_precomputed_embeds else None)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "best_model, measures = train(model=rnn, \n",
    "                             train_iter=train_iter, \n",
    "                             val_iter=val_iter, \n",
    "                             epochs=epochs, \n",
    "                             batch_size=batch_size, \n",
    "                             learning_rate=learning_rate, \n",
    "                             clip=clip,\n",
    "                             solver=solver,\n",
    "                             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and validation loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(measures[\"Train loss\"], label=\"Train Loss\")\n",
    "plt.plot(measures[\"Val loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Loss Plot\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and validation acc\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(measures[\"Train acc\"], label=\"Train Acc\")\n",
    "plt.plot(measures[\"Val acc\"], label=\"Validation Acc\")\n",
    "plt.title(\"Accuracy Plot\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, \n",
    "             test_iter, \n",
    "             batch_size, \n",
    "             device):\n",
    "    \"\"\"\n",
    "    Evaluate model on data and calculate accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : `torch.nn.Module` \n",
    "        The model.\n",
    "    test_iter : `torchtext.data.iterator.BPTTIterator`\n",
    "        The test data iterator.\n",
    "    batch_size : `int`\n",
    "        The number of sequences per batch.\n",
    "    device : `torch.device`\n",
    "        Whether working on GPU or CPU.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    loss : `float`\n",
    "        The loss of the model on the data predicted over\n",
    "    ppl : `float`\n",
    "        The perplexity of the model on the data predicted over     \n",
    "    acc : `float`\n",
    "        The accuracy of the model on the data predicted over\n",
    "    \"\"\"    \n",
    "    # send model to device\n",
    "    model.to(device)\n",
    "    # set to evaluate mode\n",
    "    model.eval()    \n",
    "    \n",
    "    # initialise loss function: cross-entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()         \n",
    "    \n",
    "    # get new hidden state for RNN\n",
    "    hidden = model.init_hidden(batch_size, device)          \n",
    "    # accuracy stats after every epoch\n",
    "    eval_loss = eval_corr = 0   \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_iter):\n",
    "            x, y = batch.text.T, batch.target.T # [batch_size, seq_len]\n",
    "            # get validation prediction\n",
    "            preds, hidden = model(x, hidden) # [batch_size, seq_len, output_size] and [num_layers, batch_size, hidden_size]           \n",
    "            # get validaiton loss\n",
    "            loss = criterion(preds.reshape(-1, model.output_size), \n",
    "                             y.reshape(-1)) \n",
    "            # add the validation loss\n",
    "            eval_loss += loss.item()             \n",
    "            # get number of tokens correct  \n",
    "            preds = preds.argmax(2).cpu().numpy()\n",
    "            y = y.cpu().numpy()\n",
    "            eval_corr += int((preds == y).sum())         \n",
    "    loss = eval_loss / len(test_iter)            \n",
    "    ppl = math.exp(loss)            \n",
    "    acc = eval_corr / len(test_iter.dataset.examples[0].text)\n",
    "    return loss, ppl, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_ppl, train_acc = evaluate(model=best_model, \n",
    "                                            test_iter=train_iter,\n",
    "                                            batch_size=batch_size,\n",
    "                                            device=device)\n",
    "val_loss, val_ppl, val_acc = evaluate(model=best_model, \n",
    "                                      test_iter=val_iter,\n",
    "                                      batch_size=batch_size,\n",
    "                                      device=device)\n",
    "test_loss, test_ppl, test_acc = evaluate(model=best_model, \n",
    "                                         test_iter=test_iter,\n",
    "                                         batch_size=batch_size,\n",
    "                                         device=device)\n",
    "\n",
    "print(\"Train Loss: {:.4f}\".format(train_loss), \"|\", \"Train PPL: {:.4f}\".format(train_ppl), \"|\", \"Train Acc: {:.4f}\".format(train_acc))\n",
    "print(\"Val Loss: {:.4f}\".format(val_loss), \"|\", \"Val PPL: {:.4f}\".format(val_ppl), \"|\", \"Val Acc: {:.4f}\".format(val_acc))\n",
    "print(\"Test Loss: {:.4f}\".format(test_loss), \"|\", \"Test PPL: {:.4f}\".format(test_ppl), \"|\", \"Test Acc: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "* Softmax activation function for a vector $\\mathbf{z}\\in\\mathbb{R}^{N}$ is given by\n",
    "\n",
    "$$\\sigma(\\mathbf{z})_i=\\frac{e^{z_i}}{\\sum^{N}_{j=1}e^{z}_j}\\in[0,1].$$\n",
    "\n",
    "Thus this is useful for convert the output layer values pre-application of the activation function to probabilities.\n",
    "\n",
    "* Sigmoid activation function for a scalar $z\\in\\mathbb{R}$ is given by\n",
    "\n",
    "$$\\sigma(z)=\\frac{1}{1+e^{-z}}\\in[0,1].$$\n",
    "\n",
    "This can be used to convert the output value of a neural network (single output node) pre-application of the activation function to a probability. Thus used for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, \n",
    "           size, \n",
    "           device,\n",
    "           prime=\"The\", \n",
    "           top_k=None):\n",
    "    \"\"\"\n",
    "    Given a initial sequence `prime` predicted the following \n",
    "    words.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : `torch.nn.Module`\n",
    "        The recurrent network model. \n",
    "    size : `int`\n",
    "        The number of tokens to predict\n",
    "    device : `torch.device`\n",
    "        Whether working on GPU or CPU.          \n",
    "    prime : `str`\n",
    "        The starting sequence to predict from.\n",
    "    top_k : `int`\n",
    "        The number of top tokens to draw the final token from.       \n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    text : `str`\n",
    "        The prime with predicted next tokens.\n",
    "    \"\"\"\n",
    "    # send model to device\n",
    "    model.to(device)\n",
    "    # set model to evaluate mode\n",
    "    model.eval()\n",
    "            \n",
    "    # tokenize the prime\n",
    "    tokens = TEXT.preprocess(prime)\n",
    "    # get new hidden state for testing\n",
    "    hidden = model.init_hidden(batch_size=1, device=device)\n",
    "    # iterate through tokens in prime to build up hidden state\n",
    "    for token in tokens:\n",
    "        token, hidden = predict(model, token, hidden, device, top_k=top_k)\n",
    "    # append the final token to the prime\n",
    "    tokens.append(token)\n",
    "    \n",
    "    if size>1:\n",
    "        # now pass in the previous character and get a new one\n",
    "        for _ in range(size-1):\n",
    "            token, hidden = predict(model, tokens[-1], hidden, device, top_k=top_k)\n",
    "            tokens.append(token)\n",
    "            \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def predict(model, \n",
    "            token, \n",
    "            hidden, \n",
    "            device,\n",
    "            top_k=None):\n",
    "    \"\"\"\n",
    "    Given a character, predict the next character\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : `torch.nn.Module`\n",
    "        The recurrent network model.\n",
    "    token : `str`\n",
    "        The previous token for which we must predict the next.\n",
    "    hidden : `torch.Tensor` or `tuple` of `torch.Tensor`\n",
    "        \"rnn\" & \"gru\" = hidden state, [num_layers, batch_size, hidden_size],\n",
    "        \"lstm\" = tuple of hidden_state and cell_state, both [num_layers, batch_size, hidden_size].\n",
    "    device : `torch.device`\n",
    "        Whether working on GPU or CPU.             \n",
    "    top_k : `int`\n",
    "        The number of top tokens to draw the final token from.   \n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    top_token : `str`\n",
    "        The top token predicted by the model.\n",
    "    hidden : `torch.Tensor` or `tuple` of `torch.Tensor`\n",
    "        \"rnn\" & \"gru\" = hidden state, [num_layers, batch_size, hidden_size],\n",
    "        \"lstm\" = tuple of hidden_state and cell_state, both [num_layers, batch_size, hidden_size]. \n",
    "    \"\"\" \n",
    "    # encode the token to an integer\n",
    "    enc_x = torch.LongTensor([TEXT.vocab.stoi[token]])\n",
    "    # add extra dimension to indicate batch_size of 1, [batch_size=1, seq_len=1]\n",
    "    enc_x = enc_x.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # forward pass through model\n",
    "        pred, hidden = model(enc_x, hidden)\n",
    "\n",
    "    # get the character probabilities\n",
    "    p = F.softmax(pred, dim=2).data.cpu()\n",
    "    # get most likely characters\n",
    "    if top_k is None:\n",
    "        # all chars are the top chars\n",
    "        top_token = np.arange(model.output_size)\n",
    "    else:\n",
    "        # return the prob and the top_k top_tokens \n",
    "        p, top_tokens = p.topk(top_k)\n",
    "        top_tokens = top_tokens.numpy().squeeze()\n",
    "    \n",
    "    # select the likely next character with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    top_token = np.random.choice(top_tokens, p=p/p.sum())\n",
    "    # return the encoded value of the predicted char and the hidden state\n",
    "    return TEXT.vocab.itos[top_token], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(model=rnn, \n",
    "       size=40,\n",
    "       device=device,\n",
    "       prime=\"When the unicorn\", \n",
    "       top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(model=rnn, \n",
    "       size=40,\n",
    "       device=device,\n",
    "       prime=\"He opened the door\", \n",
    "       top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
