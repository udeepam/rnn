{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "* [Sequence to Sequence Learning with Neural Networks by Ben Trevtt](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)\n",
    "* [Language translation with torchtext by PyTorch](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)\n",
    "* [Attention? Attention! by Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#born-for-translation)\n",
    "* [Sequence to Sequence Learning with Neural Networks by Sutskever et al.](https://arxiv.org/abs/1409.3215)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Sequence-to-sequence (**seq2seq**) models aim to transform an input sequence (source) to a new sequence (target), where both sequences can be of arbitrary lengths. Examples of transformation tasks include machine translation between multiple languages, question-answer generation etc. The most common seq2seq models are encoder-decoder architectures using RNNs:\n",
    "\n",
    "* **Encoder:** encodes the source sentence into a single vector of fixed length - we will call this vector the **contect vector**. This vector can be thought of as an absract representation of the entire source sentence.\n",
    "\n",
    "* **Decoder:** initialised with the context vector. Learns to output the target sentence by generating it one word at a time. Early work uses the last hidden state of the encoder network as the decoder initial hidden state.\n",
    "\n",
    "\n",
    "<img src=\"assets/seq2seq.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "The above image shows an example of a translation task. The source sentence \"$\\texttt{guten morgan}$\" is first passed through the embedding layer  (yellow blocks) and used as input to the encoder (green block). We append a start of sequence \"$\\texttt{<sos>}$\" and end of sequence \"$\\texttt{<eos>}$\" token to the start and end of the sentence.\n",
    "\n",
    "At each timestep, $t$, the input to the encoder RNN is both the embedding of the current token $\\mathbf{e}(x_t)\\in\\mathbb{R}^{k}$ as well as the hidden state from the previous timestep, $\\mathbf{h}_{t-1}\\in\\mathbb{R}^{d}$, where $k$ is the embedding dimension size and $d$ is the number of features in the hidden state. The encoder RNN outputs a new hidden state $\\mathbf{h}_t$, where here we set the output vector to be the hidden state as well. The hidden state, $\\mathbf{h}_t$, can be thought of as a vector representation of the sentence so far,\n",
    "\n",
    "$$\\mathbf{h}_t=\\text{EncoderRNN}\\Big(\\mathbf{e}(x_t),\\mathbf{h}_{t-1}\\Big).$$\n",
    "\n",
    "Here we have that $X=\\{x_1,x_2,...,x_T\\}$, where $x_1=\\texttt{<sos>}$, $x_2=\\texttt{guten}$ and so on. The initial hidden state $h_0$ is usually initialised to zeros or is a learned parameter. The above is in the case where we are using a [vanilla RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) or a [GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html). In the case where we are using a [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) we also have a cell state, $\\mathbf{c}_{t-1}\\in\\mathbb{R}^{d}$,\n",
    "\n",
    "$$\\mathbf{h}_t, \\mathbf{c}_t=\\text{EncoderRNN}\\Big(\\mathbf{e}(x_t),\\mathbf{h}_{t-1},\\mathbf{c}_{t-1}\\Big).$$\n",
    "\n",
    "Once the final token $x_T$ has been passed into the RNN we use the final hidden state $\\mathbf{h}_T$ as the context vector, i.e. $\\mathbf{z}=\\mathbf{h}_T\\in\\mathbb{R}^{d}$.\n",
    "\n",
    "Now that we have the context vector we can start decoding it to get the target sentence \"$\\texttt{good morning}$\". Again we append start and end of sequence tokens to the target sentence. At each timestep, $t$, the input to the decoder RNN (blue block) is the embedding of the current token, $\\mathbf{e}'(y_t)\\in\\mathbb{R}^{k'}$, as well as the hidden state from the previous timestep, $\\mathbf{s}_{t-1}\\in\\mathbb{R}^{d}$ (we denote the hidden state of the decoder RNN as $\\mathbf{s}$),\n",
    "\n",
    "$$\\mathbf{s}_t=\\text{DecoderRNN}\\Big(\\mathbf{e}'(y_t),\\mathbf{s}_{t-1}\\Big).$$\n",
    "\n",
    "where typically $k'=k$. Although in the diagram the source embedding layer, $\\mathbf{e}$, and target embedding layer, $\\mathbf{e}'$, are both shown in yellow, they are two different embedding layers with their own parameters.\n",
    "\n",
    "We now map the hidden states, $\\mathbf{s}_t$, to words by passing it through a `Linear` layer (shown in purple) where the number of output nodes corresponds to the vocabulary of the target language,\n",
    "\n",
    "$$\\hat{y}_{t+1}=f(\\mathbf{s}_t).$$\n",
    "\n",
    "The words in the decoder are always generated one after another, with one per time-step. We always use $x_1=y_1=$\"$\\texttt{<sos>}$\" for the first input to the decoder, but for subsequent inputs, $x_{t>1}$, we will sometimes use:\n",
    "1. the ground truth word in the sequence, e.g. $x_2=y_2$ or \n",
    "2. the word predicted by our decoder, e.g. $x_2=\\hat{y}_2$. \n",
    "\n",
    "This is called teacher forcing, see a bit more info about it [here](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/). Hence we have:\n",
    "\n",
    "* Predicted target sentence is $\\hat{Y} = \\{ 0, \\hat{y}_2, ..., \\hat{y}_T \\}$, where we do not have a prediction for $\\hat{y}_1$ as we use $y_1=$\"$\\texttt{<sos>}$\" as our input, $x_1$, to predict the next word in the sequence, $y_2$.\n",
    "* Actual target sentence $Y = \\{ y_1, y_2, ..., y_T \\} = \\{\\texttt{<sos>}, \\texttt{guten}, \\texttt{morgan}, ...\\}$\n",
    "\n",
    "We will compare these two sequences to calculate our loss. We then use this loss to update all of the parameters in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "We first load our tokenizers using `Spacy`. We use Spacy because it provides strong support for tokenization in languages other than English. `torchtext` provides a `basic_english` tokenizer and supports other tokenizers for English (e.g. Moses) but for language translation - where multiple languages are required - Spacy is your best bet. \n",
    "\n",
    "To run this notebook, first install spacy and then download the raw data for the English and German Spacy tokenizers:\n",
    "\n",
    "```bash\n",
    "python -m spacy download en\n",
    "python -m spacy download de\n",
    "```\n",
    "\n",
    "We use the `torchtext.data.Field` to structure our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(tokenize=\"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True)\n",
    "\n",
    "TRG = Field(tokenize=\"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download and load the train, validation and test data from the [Multi30K dataset](https://github.com/multi30k/dataset). This is a dataset with ~$30,000$ parallel English, German and French sentences, each with ~$12$ words per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = Multi30k.splits(exts=('.de', '.en'), \n",
    "                                                  fields=(SRC, TRG))\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(val_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next build our vocabulary's for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 7854\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally for processing the data we create our iterators using a `BucketIterator` which batches examples of similar lengths together, thus minimising the amount of padding in both source and target sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits((train_data, val_data, test_data),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the seq2seq modelling\n",
    "\n",
    "### Encoder\n",
    "Our encoder RNN consists of a 2 layer LSTM as shown below\n",
    "\n",
    "<img src=\"assets/two_layer_lstm_encoder.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Notice that we do not pass an initial hidden or cell state to the RNN. This is because, as noted in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM), that if no hidden/cell state is passed to the RNN, it will automatically create an initial hidden/cell state as a tensor of all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embed_size, \n",
    "                 hidden_size, \n",
    "                 num_layers, \n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # define embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                      embedding_dim=embed_size)\n",
    "        # define LSTM       \n",
    "        self.rnn = nn.LSTM(input_size=embed_size, \n",
    "                           hidden_size=hidden_size, \n",
    "                           num_layers=num_layers, \n",
    "                           batch_first=True,\n",
    "                           dropout = dropout)\n",
    "        # define dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input : ` torch.Tensor`\n",
    "            The source sentence, [batch_size, seq_len].\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        hidden : `torch.Tensor`\n",
    "            The hidden state of the LSTM [num_layers, batch_size, hidden_size].\n",
    "        cell : `torch.Tensor`\n",
    "            The cell state of the LSTM [num_layers, batch_size, hidden_size].            \n",
    "        \"\"\"        \n",
    "        # encode the input using embedding layer\n",
    "        x = self.dropout(self.embedding(input)) # [batch_size, seq_len, embed_size]\n",
    "        # pass through LSTM\n",
    "        # x here is the hidden states from the top LSTM layer\n",
    "        x, (hidden, cell) = self.rnn(x) # x : [batch_size, seq_len, hidden_size] \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Our decoder RNN consists of a 2 layer LSTM as shown below\n",
    "\n",
    "<img src=\"assets/two_layer_lstm_decoder.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Here we are only decoding one token at a time, thus the input tokens will always have a sequence length of $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_size, \n",
    "                 embed_size, \n",
    "                 hidden_size, \n",
    "                 num_layers, \n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        #  define embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=output_size, \n",
    "                                      embedding_dim=embed_size)\n",
    "        # define LSTM\n",
    "        self.rnn = nn.LSTM(input_size=embed_size, \n",
    "                           hidden_size=hidden_size, \n",
    "                           num_layers=num_layers,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout)\n",
    "        # define fully connected layer\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        # define dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input : ` torch.Tensor`\n",
    "            The source sentence, [batch_size].\n",
    "        hidden : `torch.Tensor`\n",
    "            The hidden state of the LSTM [num_layers, batch_size, hidden_size].\n",
    "        cell : `torch.Tensor`\n",
    "            The cell state of the LSTM [num_layers, batch_size, hidden_size].             \n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        x : `torch.Tensor`\n",
    "            The predictions, [batch_size, output_size].\n",
    "        hidden : `torch.Tensor`\n",
    "            The hidden state of the LSTM [num_layers, batch_size, hidden_size].\n",
    "        cell : `torch.Tensor`\n",
    "            The cell state of the LSTM [num_layers, batch_size, hidden_size].            \n",
    "        \"\"\"          \n",
    "        # add seq_len=1 dimension\n",
    "        x = input.unsqueeze(1) # [batch_size, 1]\n",
    "        # encode the input using the embedding layer\n",
    "        x = self.dropout(self.embedding(x)) # [batch_size, 1, embed_size]\n",
    "        # pass through LSTM        \n",
    "        x, (hidden, cell) = self.rnn(x, (hidden, cell)) # [batch_size, 1, hidden_size] and [num_layers, batch_size, hidden_size]\n",
    "        # pass through fully connected layer\n",
    "        x = self.fc_out(x.squeeze(1))\n",
    "        return x, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "Our full model looks like \n",
    "\n",
    "<img src=\"assets/two_layer_seq2seq.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "For this implementation we need to ensure the number of layers and the hidden and cell state dimension sizes are equal in the Encoder and Decoder. This is not necessary but for simplicity this is what we choose to do. \n",
    "\n",
    "When decoding, at each timestep, $t$, we will predict what the token in the target sequence will be from the previous tokens decoded, $\\hat{y}_{t+1}=f(\\mathbf{s}^{n}_t)$, where $\\mathbf{s}^{n}_t$ is the hidden state from the final LSTM layer. With probability equal to the `teacher_forcing_ratio` we will use the actual ground-truth token in the target sequence, as the input to the decoder during the next timestep, $x_{t+1}=y_{t+1}$. However, with probability $1 -$ `teacher_forcing_ratio`, we will use the token that the model predicted as the next input to the model, $x_{t+1}=\\hat{y}_{t+1}$, even if it doesn't match the actual next token in the target sequence.\n",
    "\n",
    "As stated before the first input to the decoder is $x_1=y_1=$\"$\\texttt{<sos>}$\". In the batch we know how long our target sentence will be from `max_len` and so loop through that many times. The last token input to the decoder is the one **before** \"$\\texttt{<eos>}$\". The \"$\\texttt{<eos>}$\" is never input into the decoder. During each iteration of the decoder loop:\n",
    "1. $x_1=y_1=$\"$\\texttt{<sos>}$\", $\\mathbf{s}_0=\\mathbf{h}_T$ and $\\mathbf{c}^{DECODER}_0=\\mathbf{c}^{ENCODER}_T$.\n",
    "2. Pass $(x_t, \\mathbf{s}_{t-1}, \\mathbf{c}_{t-1})$ into the decoder.\n",
    "3. Receive $(\\hat{y}_{t+1}, \\mathbf{s}_t, \\mathbf{c}_t)$ from the decoder.\n",
    "4. Place prediction $\\hat{y}_{t+1}$ in out tensor of predictions $\\hat{Y}$.\n",
    "5. Decide if we are using teaching forcing:\n",
    "    * Yes, then set $x_{t+1}=y_{t+1}$\n",
    "    * No, then set $x_{t+1}=\\hat{y}_{t+1}$, where $\\hat{y}_{t+1}$ is chosen using an `argmax` over all the words.\n",
    "    \n",
    "**NOTE:** The above is achieved as the decoder loop starts at $1$ not $0$. This means the zeroth elements of our prediction vector will be all zeros.\n",
    "\n",
    "$$\\begin{align*}\n",
    " \\text{target}&=\\{\\texttt{<sos>},y_2, y_3,\\texttt{<eos>}\\}\\\\\n",
    " \\text{predictions}&=\\{0,\\hat{y}_2, \\hat{y}_3,\\hat{y}_4\\}\n",
    "\\end{align*}$$\n",
    "\n",
    "When calculating the loss we will cut off the first element of each tensor to get\n",
    "\n",
    "$$\\begin{align*}\n",
    " \\text{target}&=\\{y_2, y_3,\\texttt{<eos>}\\}\\\\\n",
    " \\text{predictions}&=\\{\\hat{y}_2, \\hat{y}_3,\\hat{y}_4\\}\n",
    "\\end{align*}$$\n",
    "\n",
    "### Side Note (not implemented here)\n",
    "One improvement to this model would be instead of the decoder only taking as input the embedded target token, $\\mathbf{e}'(y_t)$, and the previous hidden state, $\\mathbf{s}_{t-1}$, it also takes the context vector $\\mathbf{z}$,\n",
    "\n",
    "$$\\mathbf{s}_t=\\text{DecoderRNN}\\Big(\\mathbf{e}'(y_t),\\mathbf{s}_{t-1}, \\mathbf{z}\\Big).$$\n",
    "\n",
    "Note that the context vector, $\\mathbf{z}$, does not have a $t$ subscript. This means we use the same context vector returned by the encoder for every timestep in the decoder. Additionally, before we predicted the next token, $\\hat{y}_{t+1}$, with the linear layer, $f(\\cdot)$, as $\\hat{y}_{t+1}=f(\\mathbf{s}^{n}_t)$. Now, we also pass the embedding of the current token, $\\mathbf{e}'(y_t)$ and the context vector, $\\mathbf{z}$ to the linear layer,\n",
    "\n",
    "$$\\hat{y}_{t+1} = f\\Big(\\mathbf{e}'(y_t), \\mathbf{s}_t, \\mathbf{z}\\Big)$$\n",
    "\n",
    "These two modification help alleviate the information compression as suggested in [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation by Cho et al.](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "<img src=\"assets/better_seq2seq.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "        # intialise model weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, \n",
    "                X, \n",
    "                Y, \n",
    "                teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ` torch.Tensor`\n",
    "            The source sentence, [batch_size, seq_len].\n",
    "        Y : ` torch.Tensor`\n",
    "            The target sentence, [batch_size, seq_len].            \n",
    "        teacher_forcing_ratio : `float`\n",
    "            The probability to use teacher forcing, \n",
    "            i.e. if 0.5 then we use the ground-truth inputs 75% of the time.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        preds : `torch.Tensor`\n",
    "            The predictions, [batch_size, seq_len, target_vocab_size].       \n",
    "        \"\"\"          \n",
    "        batch_size = X.size(0)\n",
    "        max_len = Y.size(1)\n",
    "        Y_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        preds = torch.zeros(batch_size, max_len, Y_vocab_size).to(self.device)\n",
    "        \n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(X)\n",
    "        \n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = Y[:, 0]\n",
    "        for t in range(1, max_len):\n",
    "            # pass through decoder\n",
    "            pred, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # place predictions in a tensor holding predictions\n",
    "            preds[:, t].copy_(pred)\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = pred.argmax(1) \n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = Y[:, t] if teacher_force else top1\n",
    "        return preds\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialise the weights and biases of the network.\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.uniform_(param.data, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We use the `nn.CrossEntropyLoss` function which calculates both the log softmax and the negative log-likelihood of our predictions. The loss function calculates the average loss per token. By passing the index of the $\\texttt{<pad>}$ token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          train_iter, \n",
    "          val_iter, \n",
    "          epochs, \n",
    "          learning_rate, \n",
    "          clip,\n",
    "          device):\n",
    "    \"\"\"\n",
    "    Fit model with the training data and validate using the validation data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : `torch.nn.Module` \n",
    "        The model.\n",
    "    train_iter : `torchtext.data.iterator.BucketIterator`\n",
    "        The training data iterator.\n",
    "    val_iter : `torchtext.data.iterator.BucketIterator`\n",
    "        The validation data iterator.\n",
    "    learning_rate : `float`\n",
    "        The learning rate.\n",
    "    clip : `float`\n",
    "        Value for clipping the gradients by.\n",
    "    device : `torch.device`\n",
    "        Whether working on GPU or CPU.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    measures : `dict` of `list`s\n",
    "        Dictionary of training and validation \n",
    "        accuracies and losses for each epoch.\n",
    "    \"\"\"  \n",
    "    # send model to gpu or cpu\n",
    "    model.to(device)\n",
    "    # set model to train mode    \n",
    "    model.train()        \n",
    "\n",
    "    # initialise optimiser\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # initialise loss function: cross-entropy loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi[TRG.pad_token])           \n",
    "    \n",
    "    measures = defaultdict(list)\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_val = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        train_loss = train_corr = 0.\n",
    "\n",
    "        for i, batch in enumerate(train_iter):          \n",
    "            x, y = batch.src.T, batch.trg.T  # [batch_size, seq_len]\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            # forward Propogation: get prediction from model\n",
    "            preds = model(x, y) # [batch_size, seq_len, output_size]\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = criterion(preds[:, 1:].reshape(-1, model.decoder.output_size), \n",
    "                             y[:, 1:].reshape(-1))              \n",
    "       \n",
    "            # backpropogation: calculating gradients\n",
    "            loss.backward()\n",
    "            # prevent the exploding gradient problem in RNNs/LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            # update weights\n",
    "            optimiser.step()\n",
    "            \n",
    "            # append the train loss\n",
    "            train_loss += loss.item()*x.size(0)*x.size(1)      \n",
    "            # get number of tokens correct \n",
    "            preds = preds.argmax(2).cpu().numpy()[:, 1:]\n",
    "            y = y.cpu().numpy()[:, 1:]\n",
    "            train_corr += int((preds == y).sum()) \n",
    "            \n",
    "        # set model to evaluate mode\n",
    "        model.eval()         \n",
    "        # monitor the validation loss\n",
    "        val_loss = val_corr = 0.        \n",
    "        with torch.no_grad():\n",
    "            for j, batch in enumerate(val_iter):                    \n",
    "                x, y = batch.src.T, batch.trg.T # [batch_size, seq_len]\n",
    "                # get validation prediction\n",
    "                preds = model(x, y, 0) # turn off teacher forcing          \n",
    "                # get validaiton loss\n",
    "                loss = criterion(preds[:, 1:].reshape(-1, model.decoder.output_size), \n",
    "                                 y[:, 1:].reshape(-1)) \n",
    "                # add the validation loss\n",
    "                val_loss += loss.item()*x.size(0)*x.size(1)      \n",
    "                # get number of tokens correct  \n",
    "                preds = preds.argmax(2).cpu().numpy()[:, 1:]\n",
    "                y = y.cpu().numpy()[:, 1:]\n",
    "                val_corr += int((preds == y).sum())\n",
    "                \n",
    "        # put model to train mode\n",
    "        model.train()      \n",
    "        \n",
    "        # save losses from epoch\n",
    "        train_loss /= len(train_iter.dataset.examples)\n",
    "        val_loss /= len(val_iter.dataset.examples)\n",
    "        measures[\"Train loss\"].append(train_loss)\n",
    "        measures[\"Val loss\"].append(val_loss)\n",
    "        # save accuracies from epoch\n",
    "        train_corr /= len(train_iter.dataset.examples)\n",
    "        val_corr /= len(val_iter.dataset.examples)      \n",
    "        measures[\"Train acc\"].append(train_corr)\n",
    "        measures[\"Val acc\"].append(val_corr)        \n",
    "        \n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs), \"|\", \"Time Taken: {:,.4f} seconds\".format(time.time()-start_time)) \n",
    "        print(\"\\tTrain Loss: {:.4f}\".format(train_loss), \"|\", \"Val Loss: {:.4f}\".format(val_loss))\n",
    "        print(\"\\tTrain PPL:  {:.4f}\".format(math.exp(train_loss)), \"|\", \"Val PPL:  {:.4f}\".format(math.exp(val_loss)))\n",
    "        print(\"\\tTrain Acc:  {:.4f}\".format(train_corr), \"|\", \"Val Acc:  {:.4f}\".format(val_corr)) \n",
    "\n",
    "        # save best model\n",
    "        if measures[\"Val loss\"][-1] < best_val:\n",
    "            best_val = measures[\"Val loss\"][-1]\n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "        # early stopping condition for convergence on validation.\n",
    "        win_size = 10\n",
    "        threshold = 0.00005        \n",
    "        try:\n",
    "            curr_avg = np.mean(measures[\"Val loss\"][-1-win_size:-1])\n",
    "            prev_avg = np.mean(measures[\"Val loss\"][-win_size*2:-win_size])\n",
    "            if np.absolute(curr_avg - prev_avg) < threshold:\n",
    "                break\n",
    "        except:\n",
    "            pass \n",
    "        \n",
    "    return best_model, measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of model\n",
    "src_vocab_size = len(SRC.vocab)\n",
    "trg_vocab_size = len(TRG.vocab)\n",
    "enc_embed_size = 256\n",
    "dec_embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# hyperparameters for training\n",
    "epochs = 1\n",
    "learning_rate = 1e-3\n",
    "clip = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(7854, 256)\n",
      "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(5893, 256)\n",
      "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialsie the encoder model\n",
    "encoder = Encoder(vocab_size=src_vocab_size, \n",
    "                  embed_size=enc_embed_size, \n",
    "                  hidden_size=hidden_size, \n",
    "                  num_layers=num_layers, \n",
    "                  dropout=enc_dropout)\n",
    "# initialise the decoder model \n",
    "decoder = Decoder(output_size=trg_vocab_size, \n",
    "                  embed_size=dec_embed_size, \n",
    "                  hidden_size=hidden_size, \n",
    "                  num_layers=num_layers, \n",
    "                  dropout=dec_dropout)\n",
    "# initialise seq2seq model\n",
    "model = Seq2Seq(encoder=encoder,\n",
    "                decoder=decoder,\n",
    "                device=device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 | Time Taken: 2.5251 seconds\n",
      "\tTrain Loss: 0.4790 | Val Loss: 5.4685\n",
      "\tTrain PPL:  1.6145 | Val PPL:  237.1025\n",
      "\tTrain Acc:  0.0000 | Val Acc:  0.0651\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "best_model, measures = train(model=model, \n",
    "                             train_iter=train_iter, \n",
    "                             val_iter=val_iter, \n",
    "                             epochs=epochs, \n",
    "                             learning_rate=learning_rate, \n",
    "                             clip=clip,\n",
    "                             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and validation loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(measures[\"Train loss\"], label=\"Train Loss\")\n",
    "plt.plot(measures[\"Val loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Loss Plot\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and validation acc\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(measures[\"Train acc\"], label=\"Train Acc\")\n",
    "plt.plot(measures[\"Val acc\"], label=\"Validation Acc\")\n",
    "plt.title(\"Accuracy Plot\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, \n",
    "             test_iter, \n",
    "             device):\n",
    "    \"\"\"\n",
    "    Evaluate model on data and calculate accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : `torch.nn.Module` \n",
    "        The model.\n",
    "    test_iter : `torchtext.data.iterator.BPTTIterator`\n",
    "        The test data iterator.\n",
    "    device : `torch.device`\n",
    "        Whether working on GPU or CPU.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    acc : `float`\n",
    "        The accuracy of the model on the data predicted over\n",
    "    \"\"\"    \n",
    "    # send model to device\n",
    "    model.to(device)\n",
    "    # set to evaluate mode\n",
    "    model.eval()    \n",
    "            \n",
    "    # accuracy stats after every epoch\n",
    "    eval_loss = eval_corr = 0   \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_iter):\n",
    "            x, y = batch.src.T, batch.trg.T # [batch_size, seq_len]\n",
    "            # get validation prediction\n",
    "            preds = model(x, y, 0)  # turn off teacher forcing  \n",
    "            # get validaiton loss\n",
    "            loss = criterion(preds[:, 1:].reshape(-1, model.decoder.output_size), \n",
    "                             y[:, 1:].reshape(-1)) \n",
    "            # add the evaluation loss\n",
    "            eval_loss += loss.item()*x.size(0)*x.size(1)                \n",
    "            # get number of tokens correct  \n",
    "            preds = preds.argmax(2).cpu().numpy()[:, 1:]\n",
    "            y = y.cpu().numpy()[:, 1:]\n",
    "            eval_corr += int((preds == y).sum())     \n",
    "    loss = eval_loss / len(test_iter.dataset.examples)            \n",
    "    ppl = math.exp(loss)\n",
    "    acc = eval_corr / len(test_iter.dataset.examples)\n",
    "    return loss, ppl, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_ppl, train_acc = evaluate(model=best_model, \n",
    "                                            test_iter=train_iter, \n",
    "                                            device=device)\n",
    "val_loss, val_ppl, vac_acc = evaluate(model=best_model, \n",
    "                                      test_iter=val_iter, \n",
    "                                      device=device)\n",
    "test_loss, test_ppl, test_acc = evaluate(model=best_model, \n",
    "                                         test_iter=test_iter,  \n",
    "                                         device=device)\n",
    "\n",
    "print(\"Train Loss: {:.4f}\".format(train_loss), \"|\", \"Train PPL: {:.4f}\".format(train_ppl), \"|\", \"Train Acc: {:.4f}\".format(train_acc))\n",
    "print(\"Val Loss: {:.4f}\".format(val_loss), \"|\", \"Val PPL: {:.4f}\".format(val_ppl), \"|\", \"Val Acc: {:.4f}\".format(val_acc))\n",
    "print(\"Test Loss: {:.4f}\".format(test_loss), \"|\", \"Test PPL: {:.4f}\".format(test_ppl), \"|\", \"Test Acc: {:.4f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
