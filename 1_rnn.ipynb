{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [CS224n: Natural Language Processing with Deep\n",
    "Learning](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes05-LM_RNN.pdf)\n",
    "* [NLP with DL CS224N Lecture 7](https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture07-fancy-rnn.pdf)\n",
    "* [Vanishing And Exploding Gradient Problems by Jefkine](https://www.jefkine.com/general/2018/05/21/2018-05-21-vanishing-and-exploding-gradient-problems/)\n",
    "* [Why LSTMs Stop Your Gradients From Vanishing: A View from the Backwards Pass by weberna](https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html)\n",
    "* [Neural Network (2): RNN and Problems of Exploding/Vanishing Gradient by Liyan Xu](https://liyanxu.blog/2018/11/01/rnn-exploding-vanishing-gradient/)\n",
    "* [Understanding LSTM Networks by colah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Einsum is all you need - Einstein summation in deep learning by Tim Rockt√§schel](https://rockt.github.io/2018/04/30/einsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN\n",
    "\n",
    "\n",
    "Vanilla recurrent neural networks (RNNs) are a class of neural networks that allow for modelling over sequential vectors. A few examples are shown below taken from [Andrej Karpathy's blog](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "<img src=\"assets/types_of_networks.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "We now discuss a vanilla RNN module as depicted in the image below\n",
    "\n",
    "<img src=\"assets/rnn_unroll.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "At each timestep, $t$, the RNN module takes as input the previous hidden state vector $\\mathbf{h}_{t-1}\\in\\mathbb{R}^{d}$ and an input vector $\\mathbf{x}_t\\in\\mathbb{R}^{k}$. It produces an output vector, $\\hat{\\mathbf{y}}_t\\in\\mathbb{R}^{m}$ (which is usually as depicted in the image above is the updated hidden state $\\hat{\\mathbf{y}}_t=\\mathbf{h}_t$) and passes the updated hidden state, $\\mathbf{h}_t$, as input to the RNN for the next timestep (shown on the left). This can be \"unrolled\" to easier visualise the behaviour of the RNN (as seen on the right). \n",
    "\n",
    "<img src=\"assets/rnn_internal.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "The internal workings of a vanilla RNN is shown above. The update to the hidden state of the RNN is given by,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{h}_t&=\\text{tanh}\\Big(\\mathbf{W}_h\\mathbf{h}_{t-1}+\\mathbf{W}_x\\mathbf{x}_t + \\mathbf{b}\\Big)\\\\\n",
    "&=\\text{tanh}\\Big(\\mathbf{W}[\\mathbf{h}_{t-1};\\mathbf{x}_t] + \\mathbf{b}\\Big),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $[\\mathbf{h}_{t-1};\\mathbf{x}_t]\\in\\mathbb{R}^{(d+k)}$ is the concatenation of vectors $\\mathbf{h}_{t-1}$ and $\\mathbf{x}_t$, $\\mathbf{W}\\in\\mathbb{R}^{d\\times(d+k)}$, $\\mathbf{W}_h\\in\\mathbb{R}^{d\\times d}$, $\\mathbf{W}_x\\in\\mathbb{R}^{d\\times k}$ are all weight matrices and $\\mathbf{b}\\in\\mathbb{R}^{d}$ is the bias vector. \n",
    "\n",
    "As noted previously, the output vector in the above images is the updated hidden state, $\\hat{\\mathbf{y}}_t=\\mathbf{h}_t$. However, this may not always be the case, for example we could add a single layer perceptron such that the output vector could be \n",
    "\n",
    "$$\\hat{\\mathbf{y}}_t=g\\Big(\\mathbf{W}_y\\mathbf{h}_t\\Big),$$\n",
    "\n",
    "where $g(\\cdot)$ is an activation function such as the $\\texttt{softmax}$ function and $\\mathbf{W}_y\\in\\mathbb{R}^{m\\times d}$ is another weight matrix. This is typical when using RNNs for language modelling so as to map the hidden state back to the vocabulary.\n",
    "\n",
    "Let our RNN be given by the function $f^{\\text{RNN}}_\\theta(\\mathbf{x}_t,\\mathbf{h}_{t-1})=(\\hat{\\mathbf{y}}_t,\\mathbf{h}_t)$. Then if we have a sequence of inputs, $\\mathbf{X}=[\\mathbf{x}_1,\\mathbf{x}_2,...,\\mathbf{x}_T]$, we can unfold the RNN to obtain,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{unfold}\\Big(f^{\\text{RNN}}_\\theta, \\mathbf{X}, \\mathbf{h}_0\\Big)&=\\Big[f_\\theta(\\mathbf{x}_1,\\mathbf{h}_0),f_\\theta(\\mathbf{x}_2,\\mathbf{h}_1),...,f_\\theta(\\mathbf{x}_T,\\mathbf{h}_{T-1})\\Big]\\\\\n",
    "&=\\Big[(\\hat{\\mathbf{y}}_1,\\mathbf{h}_1),(\\hat{\\mathbf{y}}_2,\\mathbf{h}_2),...,(\\hat{\\mathbf{y}}_T,\\mathbf{h}_T)\\Big]\n",
    "\\end{align*}.$$\n",
    "\n",
    "### Backpropagation Through Time (BPTT)\n",
    "\n",
    "The discrepency between the output vector $\\hat{\\mathbf{y}}_t\\in\\mathbb{R}^{m}$ and the desired label vector $\\mathbf{y}_t\\in\\mathbb{R}^{m}$ is evaluated by a loss function across all $T$ timesteps as\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{x}_1,...,\\mathbf{x}_T,\\mathbf{y}_1,...,\\mathbf{y}_T)=\\frac{1}{T}\\sum_{t=1}^{T}\\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t),$$\n",
    "\n",
    "where $\\ell(\\cdot, \\cdot)$ is typically the cross entropy loss, $\\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)=-\\sum_{j=1}^{m}y_{t,j}\\log\\hat{y}_{t,j}$.\n",
    "\n",
    "The goal is now to calculate the gradients of our loss function w.r.t. the parameters $\\mathbf{b}$, $\\mathbf{W}_h$, $\\mathbf{W}_x$ and $\\mathbf{W}_y$:\n",
    "\n",
    "1. The derivative of the loss w.r.t. $\\mathbf{b}$ \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}}&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\mathbf{b}}\\\\\n",
    "&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\hat{\\mathbf{y}}_t}\\frac{\\partial \\hat{\\mathbf{y}}_t}{\\partial \\mathbf{h}_t}\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{b}}.\n",
    "\\end{align*}$$\n",
    "\n",
    "2. The derivative of the loss w.r.t. $\\mathbf{W}_y$ \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_y}&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\mathbf{W}_y}\\\\\n",
    "&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\hat{\\mathbf{y}}_t}\\frac{\\partial \\hat{\\mathbf{y}}_t}{\\partial \\mathbf{W}_y}.\n",
    "\\end{align*}$$\n",
    "\n",
    "3. The derivative of the loss w.r.t. $\\mathbf{W}_h$ \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_h}&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\mathbf{W}_h}\\\\\n",
    "&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\hat{\\mathbf{y}}_t}\\frac{\\partial \\hat{\\mathbf{y}}_t}{\\partial \\mathbf{h}_t}\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_h}\\\\\n",
    "&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\hat{\\mathbf{y}}_t}\\frac{\\partial \\hat{\\mathbf{y}}_t}{\\partial \\mathbf{h}_t}\\sum^{t}_{i=1}\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_i}\\frac{\\partial \\mathbf{h}_i}{\\partial \\mathbf{W}_h}\\\\\n",
    "&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\hat{\\mathbf{y}}_t}\\frac{\\partial \\hat{\\mathbf{y}}_t}{\\partial \\mathbf{h}_t}\\sum^{t}_{i=1}\\Big(\\prod^{t-1}_{j=i}\\frac{\\partial \\mathbf{h}_{j+1}}{\\partial \\mathbf{h}_{j}}\\Big)\\frac{\\partial \\mathbf{h}_i}{\\partial \\mathbf{W}_h},\n",
    "\\end{align*}$$ \n",
    "\n",
    "where we have used the multivariate chain rule such that for a function $z=f\\big(x(t),y(t)\\big)$, its derivative is given by $\\frac{\\partial z}{\\partial t}=\\frac{\\partial z}{\\partial x}\\frac{\\partial x}{\\partial t}+\\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial t}$. Hence we have that\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_h}&= \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_h}\\frac{\\partial \\mathbf{W}_t}{\\partial \\mathbf{W}_h}+ \\frac{\\partial \\mathbf{h}_{t}}{\\partial \\mathbf{h}_{t-1}}\\frac{\\partial \\mathbf{h}_{t-1}}{\\partial \\mathbf{W}_h}\\\\\n",
    "&=\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_h}+ \\frac{\\partial \\mathbf{h}_{t}}{\\partial \\mathbf{h}_{t-1}}\\big(\\frac{\\partial \\mathbf{h}_{t-1}}{\\partial \\mathbf{W}_h}\\frac{\\partial \\mathbf{W}_{h}}{\\partial \\mathbf{W}_h}+\\frac{\\partial \\mathbf{h}_{t-1}}{\\partial \\mathbf{h}_{t-2}}\\frac{\\partial \\mathbf{h}_{t-2}}{\\partial \\mathbf{W}_h}\\big)\\\\\n",
    "&=\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_t}\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_h}+ \\frac{\\partial \\mathbf{h}_{t}}{\\partial \\mathbf{h}_{t-1}}\\frac{\\partial \\mathbf{h}_{t-1}}{\\partial \\mathbf{W}_h}+\\frac{\\partial \\mathbf{h}_{t}}{\\partial \\mathbf{h}_{t-2}}\\frac{\\partial \\mathbf{h}_{t-2}}{\\partial \\mathbf{W}_h}\\\\\n",
    "&\\vdots\\\\\n",
    "&=\\sum^{t}_{i=1}\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_i}\\frac{\\partial \\mathbf{h}_i}{\\partial \\mathbf{W}_h}.\n",
    "\\end{align*}$$\n",
    "\n",
    "Additionally we can use the chain rule again on $\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_i}$ which involves the product of Jacobians $\\frac{\\partial \\mathbf{h}_{j+1}}{\\partial \\mathbf{h}_{j}}$ over subsequences linking an event at time $t$ and one at time $i$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_i}=\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}}\\frac{\\partial \\mathbf{h}_{t-1}}{\\partial \\mathbf{h}_{t-2}}...\\frac{\\partial \\mathbf{h}_{i+1}}{\\partial \\mathbf{h}_i}=\\prod^{t-1}_{j=i}\\frac{\\partial \\mathbf{h}_{j+1}}{\\partial \\mathbf{h}_{j}},$$\n",
    "\n",
    "\n",
    "\n",
    "4. The derivative of the loss w.r.t. $\\mathbf{W}_x$ similar to the above\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_x}&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\mathbf{W}_x}\\\\\n",
    "&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\hat{\\mathbf{y}}_t}\\frac{\\partial \\hat{\\mathbf{y}}_t}{\\partial \\mathbf{h}_t}\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_x}\\\\\n",
    "&=\\frac{1}{T}\\sum^{T}_{t=1}\\frac{\\partial \\ell(\\mathbf{y}_t,\\hat{\\mathbf{y}}_t)}{\\partial \\hat{\\mathbf{y}}_t}\\frac{\\partial \\hat{\\mathbf{y}}_t}{\\partial \\mathbf{h}_t}\\sum^{t}_{i=1}\\Big(\\prod^{t-1}_{j=i}\\frac{\\partial \\mathbf{h}_{j+1}}{\\partial \\mathbf{h}_{j}}\\Big)\\frac{\\partial \\mathbf{h}_i}{\\partial \\mathbf{W}_x}.\n",
    "\\end{align*}$$ \n",
    "\n",
    "### Advantages and Disadvantages of RNNs\n",
    "\n",
    "The advantages of RNNs are that:\n",
    "1. They can process input sequences of any length;\n",
    "2. The model size does not increase for longer input sequence lengths;\n",
    "3. Computation for step $t$ can in theory use information from many steps back;\n",
    "4. The same weights are applied to every timestep of the input, so there is symmetry in how inputs are processed.\n",
    "\n",
    "On the otherhand, the disadvantages of RNNs are:\n",
    "1. Computation is slow - as it is sequential it cannot be paralellised;\n",
    "2. In practice, it is difficult to access information from many steps back due to problems like vanishing and exploding gradients.\n",
    "\n",
    "### Vanishing and Exploding Gradients\n",
    "\n",
    "The product of Jacobians when evaluated gives\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_i}=\\prod^{t-1}_{j=i}\\frac{\\partial \\mathbf{h}_{j+1}}{\\partial \\mathbf{h}_{j}}=\\prod^{t-1}_{j=i}\\mathbf{W}_h^{T}\\text{diag}\\Big(\\tanh'\\big(\\mathbf{W}_h\\mathbf{h}_{j}+ \\mathbf{W}_x\\mathbf{x}_j+\\mathbf{b}\\big)\\Big).$$\n",
    "\n",
    "Lets look at the L2 matrix norms associated with these Jacobians\n",
    "\n",
    "$$\\bigg\\Vert\\frac{\\partial \\mathbf{h}_{j+1}}{\\partial \\mathbf{h}_{j}}\\bigg\\Vert_2\\leq\\big\\Vert\\mathbf{W}^{T}_h\\big\\Vert_2\\big\\Vert\\text{diag}\\Big(\\tanh'\\big(\\mathbf{W}_h\\mathbf{h}_{j}+ \\mathbf{W}_x\\mathbf{x}_j+\\mathbf{b}\\big)\\Big)\\big\\Vert_2,$$\n",
    "\n",
    "where we use the Cauchy-Schwarz inequality, $\\Vert\\mathbf{a}^{T}\\mathbf{b}\\Vert_2\\leq\\Vert\\mathbf{a}\\Vert_2\\Vert\\mathbf{b}\\Vert_2$. \n",
    "\n",
    "We set $\\gamma_w$, the largest eigenvalue associated with $\\big\\Vert\\mathbf{W}^{T}_h\\big\\Vert_2$ to be its upper bound, while $\\gamma_h$, the largest eignvalue associated with $\\big\\Vert\\text{diag}\\Big(\\tanh'\\big(\\mathbf{W}_h\\mathbf{h}_{j}+ \\mathbf{W}_x\\mathbf{x}_j+\\mathbf{b}\\big)\\Big)\\big\\Vert_2$ as its corresponding upper bound. Depending on the activation function the upperbound $\\gamma_h$ can be:\n",
    "1. $\\gamma_h=1$ for `tanh` activation function,\n",
    "2. $\\gamma_h=\\frac{1}{4}$ for `sigmoid` activation function.\n",
    "\n",
    "This means we can write that \n",
    "\n",
    "$$\\bigg\\Vert\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_i}\\bigg\\Vert_2=\\bigg\\Vert\\prod^{t-1}_{j=i}\\frac{\\partial \\mathbf{h}_{j+1}}{\\partial \\mathbf{h}_{j}}\\bigg\\Vert_2\\leq(\\gamma_w\\gamma_h)^{t-i}.$$\n",
    "\n",
    "As the sequence gets longer (i.e the distance between $t$ and $i$ increases), then the value of $\\gamma$ will determine if the gradient either explodes or vanishes. Hence if $\\gamma<1$ the gradients tend to vanish, while if $\\gamma>1$ the gradients tend to explode.\n",
    "\n",
    "\n",
    "### Solutions for Exploding Gradients\n",
    "\n",
    "1. **Truncated Backpropagation Through Time (TBPTT):** a modified version of the BPTT algorithm where we have a maximum number of timesteps, $n$, along which the error can be propogated. This means instead of having $\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_i}=\\prod^{t-1}_{j=i}\\frac{\\partial \\mathbf{h}_{j+1}}{\\partial \\mathbf{h}_{j}}$, we have $\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_i}=\\prod^{t-1}_{j=t-n}\\frac{\\partial \\mathbf{h}_{j+1}}{\\partial \\mathbf{h}_{j}}$, where $n<<i$.\n",
    "\n",
    "2. **Clipping Gradients:** the gradients are clipped to be within a specific range, preventing gradients exploding.\n",
    "\n",
    "\n",
    "### Solutions for Vanishing Gradients\n",
    "\n",
    "1. **Long Short-Term Memory (LSTM):** a more sophisticated version of a vanilla RNN with additional gates to prevent vanishing gradients. Will be discussed below in more detail.\n",
    "\n",
    "2. **Gated Recurrent Unit (GRU):** again a more sophisticated version of vanilla RNNs but with fewer gates than LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we show code for a batched vanilla RNN module.\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size \n",
    "    \n",
    "        self.W = torch.rand((hidden_size, hidden_size+input_size))\n",
    "        self.b = torch.zeros(hidden_size, 1)\n",
    "        \n",
    "    def f(self, x, h):\n",
    "        \"\"\"\n",
    "        x : [batch_size, input_size]\n",
    "        h : [batch_size, hidden_size]        \n",
    "        \"\"\"\n",
    "        x = torch.cat([h, x], dim=1) # [batch_size, hidden_size+input_size]\n",
    "        h = torch.tanh(torch.einsum(\"ij,kj->ki\", [self.W, x]) + self.b.T)\n",
    "        y_hat = h\n",
    "        return y_hat, h\n",
    "        \n",
    "    def forward(self, X, h):\n",
    "        \"\"\"\n",
    "        x : [batch_size, seq_len, input_size]\n",
    "        h : [batch_size, hidden_size]\n",
    "        Y_hat : [batch_size, seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "        Y_hat = torch.zeros(batch_size, X.size(1), hidden_size)\n",
    "        # unroll\n",
    "        for i in range(X.size(1)):\n",
    "            y_hat, h = self.f(X[:,i,:], h)\n",
    "            Y_hat[:,i,:] = y_hat\n",
    "        return Y_hat, h\n",
    "        \n",
    "    def init_h(self, batch_size, hidden_size):\n",
    "        return torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "input_size = 5\n",
    "hidden_size = 3\n",
    "\n",
    "rnn = VanillaRNN(input_size=input_size, hidden_size=hidden_size)\n",
    "h = rnn.init_h(batch_size=batch_size, hidden_size=hidden_size)\n",
    "X = torch.randn(batch_size, seq_len, input_size)\n",
    "Y_hat, h = rnn.forward(X, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked RNNs\n",
    "\n",
    "We can also stack RNNs as shown below\n",
    "\n",
    "<img src=\"assets/stacked_rnn.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Here the first RNN layer takes as input the input vector, $\\mathbf{x}_t$, and the initial first layer hidden state $\\mathbf{h}_0^{(1)}$. The $i^{\\text{th}}$ RNN layer takes as input the hidden state from the previous RNN layer, e.g. $\\mathbf{h}^{(i-1)}_1$ and the initial current layer hidden state $\\mathbf{h}_0^{(i)}$.\n",
    "\n",
    "The output of the stacked RNN, $\\hat{\\mathbf{y}}_t$, is the result of propagating the input through all the $K$ layers,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{h}^{(1)}_t&=\\text{tanh}\\Big(\\mathbf{W}^{(1)}_h\\mathbf{h}^{(1)}_{t-1}+\\mathbf{W}^{(1)}_x\\mathbf{x}_t + \\mathbf{b}\\Big)\\\\\n",
    "\\mathbf{h}^{(i)}_t&=\\text{tanh}\\Big(\\mathbf{W}^{(i)}_h\\mathbf{h}^{(i)}_{t-1}+\\mathbf{W}^{(i)}_x\\mathbf{h}^{(i-1)}_{t-1} + \\mathbf{b}\\Big),\n",
    "\\end{align*}$$\n",
    "\n",
    "and then using only the hidden state from the final RNN layer,\n",
    "\n",
    "$$\\hat{\\mathbf{y}}_t=g\\Big(\\mathbf{W}_y\\mathbf{h}^{(K)}_t\\Big).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNNs\n",
    "\n",
    "With a bidirectional RNN, we have two RNNs in each layer. A forward RNN going over the embedded sentence from left to right (shown below in green), and a backward RNN going over the embedded sentence from right to left (teal).\n",
    "\n",
    "<img src=\"assets/bidirectional.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Thus the mathematical formulation behind a bidirectional RNN is given as,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\overrightarrow{\\mathbf{h}}_t&=\\text{tanh}\\Big(\\overrightarrow{\\mathbf{W}}_h\\overrightarrow{\\mathbf{h}}_{t-1}+\\overrightarrow{\\mathbf{W}}_x\\mathbf{x}_t + \\overrightarrow{\\mathbf{b}}\\Big)\\\\\n",
    "\\overleftarrow{\\mathbf{h}}_t&=\\text{tanh}\\Big(\\overleftarrow{\\mathbf{W}}_h\\overleftarrow{\\mathbf{h}}_{t-1}+\\overleftarrow{\\mathbf{W}}_x\\mathbf{x}_t + \\overleftarrow{\\mathbf{b}}\\Big),\n",
    "\\end{align*}$$\n",
    "\n",
    "with the output vector given as ,\n",
    "\n",
    "$$\\hat{\\mathbf{y}}_t=g\\Big(\\mathbf{W}_y[\\overrightarrow{\\mathbf{h}}_t;\\overleftarrow{\\mathbf{h}}_t]\\Big).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory\n",
    "\n",
    "LSTMs are a type of RNN proposed by  proposed by [Hochreiter and Schmidhuber in 1997](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf) as a\n",
    "solution to the vanishing gradients problem. In addition to a hidden state, $\\mathbf{h}_t\\in\\mathbb{R}^{d}$, that is taken as input the LSTM also has a cell state, $\\mathbf{c}_t\\in\\mathbb{R}^{d}$. The cell state stores long-term information and the LSTM can remove, add and read information from the cell state, which is achieved through special gates in the internal structure of the LSTM depicted below\n",
    "\n",
    "<img src=\"assets/lstm_internal.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "All the gates take as input the previous hidden state $\\mathbf{h}_{t-1}$ and the current input state $\\mathbf{x}_t$ and through matrix-vector multiplication and a non-linear transformation output a vector with the same dimensions as the cell state $\\mathbf{c}_{t-1}$.\n",
    "\n",
    "### 1. Forget Gate\n",
    "\n",
    "<img src=\"assets/forget_gate.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Through a sigmoid activation function outputs a vector $\\mathbf{f}_t$ whose elements take values in $[0,1]$. Intuitively the forget gate decides what information should be kept or forgotten in the previous cell state $\\mathbf{c}_{t-1}$. This is achieved through element-wise multiplication of $\\mathbf{f}_t$ with the previous cell state $\\mathbf{c}_{t-1}$. A value of $0$ in $\\mathbf{f}_t$ corresponds to \"fully forget\" the corresponding element in $\\mathbf{c}_{t-1}$, whereas a value of $1$ in $\\mathbf{f}_t$ corresponds to \"fully remember\" the corresponding element in $\\mathbf{c}_{t-1}$,\n",
    "\n",
    "$$\\mathbf{f}_t=\\sigma\\Big(\\mathbf{W}_f[\\mathbf{h}_{t-1};\\mathbf{x}_t]+\\mathbf{b}_f\\Big).$$\n",
    "\n",
    "### 2. New Memory Cell Gate\n",
    "\n",
    "<img src=\"assets/input_gate.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Through the `tanh` activation function outputs a vector $\\tilde{\\mathbf{C}}_t$ whose elements take values in $[-1,+1]$. Intuitively, the gate uses the current input $\\mathbf{x}_t$ and previous hidden state $\\mathbf{h}_{t-1}$ to generate a new memory $\\tilde{\\mathbf{C}}_t$, which includes aspects of the new input $\\mathbf{x}_t$, that could be added to the previous cell state $\\mathbf{c}_{t-1}$,\n",
    "\n",
    "$$\\tilde{\\mathbf{C}}_t=\\text{tanh}\\Big(\\mathbf{W}_C[\\mathbf{h}_{t-1};\\mathbf{x}_t]+\\mathbf{b}_C\\Big).$$\n",
    "\n",
    "### 3. Input Gate\n",
    "\n",
    "<img src=\"assets/input_gate.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Through the sigmoid activation function outputs a vector $\\mathbf{i}_t$, whose elements take values in $[0,1]$. Intuitively the input gate decides which values in the previous cell state $\\mathbf{c}_{t-1}$ we will update. This is achieved through element-wise multiplication of $\\mathbf{i}_t$ with the candidate values from the new memory cell gate $\\tilde{\\mathbf{C}}_t$. The resulting vector is then added element-wise to the previous cell state $\\mathbf{c}_{t-1}$. A value of $0$ in $\\mathbf{i}_t$ corresponds to \"not important - forget\" the corresponding element in $\\tilde{\\mathbf{C}}_{t}$ and thus do not update the corresponding element in the previous cell state $\\mathbf{c}_{t-1}$. Whereas a value of $1$ in $\\mathbf{i}_t$ corresponds to \"very important - keep\" the corresponding element in $\\tilde{\\mathbf{C}}_{t}$ and thus update the corresponding element in the previous cell state $\\mathbf{c}_{t-1}$,\n",
    "\n",
    "$$\\mathbf{i}_t=\\sigma\\Big(\\mathbf{W}_i[\\mathbf{h}_{t-1};\\mathbf{x}_t]+\\mathbf{b}_i\\Big).$$\n",
    "\n",
    "### 4. Output Gate\n",
    "\n",
    "<img src=\"assets/output_gate.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Through the sigmoid activation function outputs a vector $\\mathbf{o}_t$, whose elements take values in $[0,1]$. Intuitively, the output gate controls what part of the cell state $\\mathbf{c}_t$ are output to the hidden state $\\mathbf{h}_t$. The current cell state $\\mathbf{c}_t$ contains a lot of information that is not necessarily required to be saved in the hidden state $\\mathbf{h}_t$. The output gate makes the assessment regarding what parts of the memory $\\mathbf{c}_t$ needs to be present in the hidden state $\\mathbf{h}_t$. This is achieved through element-wise multiplication of $\\mathbf{o}_t$ with the point-wise `tanh` of the current cell state $\\mathbf{c}_t$. A value of $0$ in $\\mathbf{o}_t$ corresponds to \"not necessary - forgot\" the corresponding element in $\\tanh(\\mathbf{c}_t)$, whereas a value of $1$ in $\\mathbf{o}_t$ corresponds to \"necessary - keep\" the corresponding element in $\\tanh(\\mathbf{c}_t)$,\n",
    "\n",
    "$$\\mathbf{o}_t=\\sigma\\Big(\\mathbf{W}_o[\\mathbf{h}_{t-1};\\mathbf{x}_t]+\\mathbf{b}_o\\Big).$$\n",
    "\n",
    "### Updating Hidden State\n",
    "\n",
    "<img src=\"assets/output_gate.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "As mentioned above the update to the hidden state follows as \n",
    "\n",
    "$$\\mathbf{h}_t=\\mathbf{o}_t\\odot\\text{tanh}(\\mathbf{c}_t).$$\n",
    "\n",
    "### Updating Cell State\n",
    "\n",
    "<img src=\"assets/cell_state.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "As mentioned above the update to the cell state follows as \n",
    "\n",
    "$$\\mathbf{c}_t=\\mathbf{c}_{t-1}\\odot\\mathbf{f}_t + \\mathbf{i}_t\\odot\\tilde{\\mathbf{C}}_t.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we show code for a batched LSTM module.\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size \n",
    "    \n",
    "        self.W = torch.rand((4*hidden_size, hidden_size+input_size))\n",
    "        self.b = torch.zeros(4*hidden_size, 1)\n",
    "        \n",
    "    def f(self, x, h, c):\n",
    "        \"\"\"\n",
    "        x : [batch_size, input_size]\n",
    "        h : [batch_size, hidden_size]\n",
    "        c : [batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        x = torch.cat([h, x], dim=1) # [batch_size, hidden_size+input_size]\n",
    "        u = torch.einsum(\"ij,kj->ki\", [self.W, x]) + self.b.T # [batch_size, 4*hidden_size]\n",
    "        f = torch.sigmoid(u[:, :self.hidden_size])\n",
    "        C = torch.tanh(u[:, self.hidden_size:2*self.hidden_size])\n",
    "        i = torch.sigmoid(u[:, 2*self.hidden_size:3*self.hidden_size])\n",
    "        o = torch.sigmoid(u[:, 3*self.hidden_size:])\n",
    "        c = c*f + i*C\n",
    "        h = o*torch.tanh(c)\n",
    "        y_hat = h\n",
    "        return y_hat, h, c\n",
    "        \n",
    "    def forward(self, X, h, c):\n",
    "        \"\"\"\n",
    "        x : [batch_size, seq_len, input_size]\n",
    "        h : [batch_size, hidden_size]\n",
    "        c : [batch_size, hidden_size]\n",
    "        Y_hat : [batch_size, seq_len, hidden_size]               \n",
    "        \"\"\"\n",
    "        Y_hat = torch.zeros(batch_size, X.size(1), hidden_size)         \n",
    "        # unroll\n",
    "        for i in range(X.size(1)):\n",
    "            y_hat, h, c = self.f(X[:,i,:], h, c)\n",
    "            Y_hat[:,i,:] = y_hat\n",
    "        return Y_hat, h, c\n",
    "        \n",
    "    def init_h_c(self, batch_size, hidden_size):\n",
    "        return torch.zeros(batch_size, hidden_size), torch.zeros(batch_size, hidden_size)\n",
    "    \n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "input_size = 5\n",
    "hidden_size = 3\n",
    "\n",
    "lstm = LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "h, c = lstm.init_h_c(batch_size=batch_size, hidden_size=hidden_size)\n",
    "X = torch.randn(batch_size, seq_len, input_size)\n",
    "Y_hat, h, c = lstm.forward(X, h, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
